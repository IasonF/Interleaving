%ACM journals format:
\documentclass[prodmode,acmtodaes]{acmsmall} 
\usepackage[ruled]{algorithm2e}
\usepackage{multirow}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

%Springer DAES format below:
%\documentclass[runningheads,a4paper]{llncs}
%\usepackage{amssymb}
%\setcounter{tocdepth}{3}
%\usepackage{graphicx}

% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}

% Document starts
\begin{document}

% Page heads
\markboth{I. Filippopoulos et al.}{Data Interleaving and Memory Mapping on VLIW architectures}

% Title portion
\title{Integrated Optimization Methodology for Data Interleaving and Memory Mapping Exploration on VLIW architectures}
\author{IASON FILIPPOPOULOS
\affil{Norwegian University of Science and Technology}
NAMITA SHARMA 
\affil{Indian Institute of Technology Delhi}
FRANCKY CATTHOOR
\affil{IMEC}
PER GUNNAR KJELDSBERG
\affil{Norwegian University of Science and Technology}
PREETI PANDA
\affil{Indian Institute of Technology Delhi}
}

\begin{abstract}
This work presents a methodology for efficient exploration of data interleaving and data-to-memory mapping options for SIMD (Single Instruction Multiple Data) platform architectures.
The system architecture includes VLIW (Very Long Instruction Word) function units and a reconfigurable clustered memory. 
The scope is the reduction of the overall energy consumption by increasing the utilization of the function units and decreasing the number of memory accesses.
The presented methodology is tested using a number of benchmark applications with irregularities on their access scheme.
Potential gains are calculated based on the energy models both for the processing and the memory part of the system.
\end{abstract}

\category{D.2.2}{Design Tools and Techniques}{Design, Methodologies}
\category{B.7.1}{Types and Design Styles}{Memory Technologies, Design}
\category{B.8.2}{Performance Analysis and Design Aids}{}

\terms{Design, Algorithms, Performance}

\keywords{Data interleaving, single instruction multiple data (SIMD), system scenarios, design space exploration, memory reconfiguration}

%\acmformat{Iason Filippopoulos, Namita Sharma, Francky Catthoor, Per Gunnar Kjeldsberg, Preeti Panda, 2015. Integrated Optimization Methodology for Data Interleaving and Memory Mapping Exploration on VLIW architectures.}

%\begin{bottomstuff}
%Author's addresses: Iason Filippopoulos {and} Per Gunnar Kjeldsberg, Department of Electronics and Telecommunications, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Namita Sharma  {and} Preeti Panda, Department of Computer Science and Engineering, Indian Institute of Technology Delhi, New Delhi, India; Francky Catthoor, IMEC, Leuven, Belgium.
%\end{bottomstuff}

\maketitle

\section{Introduction}

The goal of this work is to improve both the performance and the energy consumption for data intensive applications. 
We focus on single instruction, multiple data (SIMD) architectures and deal with applications that have irregularities on their access scheme. 
SIMD architectures can potentially increase the performance of an application, providing that the utilization of them is high. 
However, applications with irregular access patterns do not provide compact sequences of data that are suitable for high utilization. 
Hence the performance is lower than expected. 
In order to improve the performance a systematic exploration of the interleaving options for application's data is needed. 

The energy consumption can be divided into two parts, namely the processing and the memory subsystem. 
The energy needed for processing depends mainly on the utilization of the FUs and any potential stalls, if the memory cannot provide data on the needed rate.
The interleaving exploration can increase the utilization of the processing subsystem and reduce time penalties for data loading.   
The energy consumption on the memory subsystem is affected by the number of memory accesses and the energy per access. 
Again, the memory architecture and the data-to-memory mapping decisions have a great impact on both the number of accesses and the energy per access.

This article is organized as follows. 
Section~\ref{sec:motivational} motivates the study of developing a methodology for optimization of the data interleaving exploration and the data-to-memory mapping. 
Section~\ref{sec:related} surveys related work on both the interleaving and memory mapping exploration.
Section~\ref{sec:methodology} presents the general work-flow and the sequence of the methodology steps.
In Section~\ref{sec:platform} the target platform is described accompanied by a detailed description of the employed SIMD architecture and the memory models, while the set of benchmarks and their characteristics are analysed in Section~\ref{sec:applications}. 
Results of applying the described exploration methodology to the targeted applications are shown in Section~\ref{sec:results}, while conclusions are drawn in Section~\ref{sec:conclusion}. 


\section{Motivational Example}
\label{sec:motivational}

A large number of papers have demonstrated the importance of the memory organization to the overall system energy consumption. 
As shown in \cite{Gonzalez1996} memory contributes around 40\% to the overall power consumption in general purpose systems. 
Especially for embedded systems, the memory subsystem accounts for up to 50\% of the overall energy consumption \cite{Che09} and the cycle-accurate simulator presented in \cite{Ben99} estimates that the energy expenditures in the memory subsystem range from 35\% up to 65\% for different architectures. 
The breakdown of power consumption for a recently implemented embedded system presented in \cite{Hul11} shows that the memory subsystem consumes more than 40\% of the leakage power on the platform. 
According to \cite{tcm}, conventional allocation and assignment of data done by regular compilers is suboptimal. 
Performance loss is caused by stalls for fetching data and data conflicts for different tasks, due to the limited size of memory and the competition between tasks. 

To illustrate the sub-optimal utilization of SIMD architectures using conventional allocation and assignment of data, the simple example of Alg.~\ref{alg:motivation} is used.
In this example, we assume that the desired result is always the sum of 4 elements from arrays \textit{A, B, C and D}. 
The access pattern shows an irregularity, as a result of the iteration index.
For every group of four sequential array elements, there is only one used for the calculation of the result and the other three are skipped.
An intuitive interleaving optimization is the interleaving of the arrays \textit{A, B, C and D}, in order to generate sequences of elements that are all useful on the calculation of the \textit{result} variable. 
A full interleaving exploration could reveal several options to produce larger sequences of array elements that are needed during the execution of Alg.~\ref{alg:motivation}.
For example, the interleaving of every fourth line within the combined array ($A\vert B\vert C\vert D$) result in a sequence of 8 accessed elements.

\begin{algorithm}[t]
\SetAlgoNoLine
\For{($i = 0; i < N; i+4$)}{
        ${result(i) = A(i) + B(i) + C(i) + D(i)}$\;
}
\caption{Motivational Example Algorithm}
\label{alg:motivation}
\end{algorithm}

The data-to-memory mapping is presented in Fig.~\ref{fig:motivation}. 
The memory architecture consists of four memory banks and the overall memory size is enough to fit the four arrays.
The conventional approach maps each array in a separate bank.
If the four arrays are interleaved, the mapping distributes the data through the banks.
The first four elements are respectively from arrays A, B, C and D and so forth.
As a result only one forth of the array A can be found on the first bank in contrast to the non-interleaving case, in which the whole array A is mapped on the first bank.

\begin{figure}
\centering
	\caption{Data-to-memory mapping for motivational example}	
	\label{fig:motivation}
	\includegraphics[scale = 0.5]{Images/motivation.eps}
\end{figure}

A quick estimation for the difference in the energy consumption between the two approaches can be calculated using a simple energy model for the system.
We can assume a system architecture with an ADD FU that performs operations over 4 words at a time and a memory to processor path that has a width of 4 words. 
Each array element is assumed to have the size of one word.
The register file at the processor can only store the iteration variable $i$ and 5 words, which are the variables result, A, B, C and D.
Without interleaving of data the number of memory accesses for each loop iteration is 4, because the elements $A(i)$, $B(i)$, $C(i)$ and $D(i)$ are stored in different memory banks.
Thus, the energy consumption for reading is 75\% lower when the data are interleaved.
The performance is also higher as there are 75\% less memory accesses.
The energy consumption due to memory leakage is also lower, as the access time for the memory and the execution time of the algorithm are shorter.

\section{Related work}
\label{sec:related}

The current work combines and expands the interleaving exploration presented in \cite{sharma2013data} and the data to memory mapping methodology presented in \cite{filippopoulos2013exploration}. Data layout optimizations aim to arrange data in memory with the objective of improving system performance and energy through means such as reduced memory access count or reduced cache misses. Several more generic data layout techniques have been explored by researchers at various levels in the memory hierarchy. But these are not well matched to our context of LTE optimisations. In particular, Cache partitioning [4], a layout technique for arrays, maps each array to different cache partitions to reduce conflicts. Kulkarni et al. [5] addressed the problem for cache miss reduction by evaluating the tiling size for arrays and merging the arrays appropriately for each loop nest. Memory Hierarchy Layer Assignment (MHLA), an optimization methodology for data memory hierarchy [6], determines for each data set an appropriate layer in the hierarchy and type of memory (single/dual port) taking data re-use into account. The strategy in [7] partitions the variables to Scratch Pad Memory and DRAM in a way that interference among different variables is minimized. To the best of our knowledge, no work has been presented yet for an efficient data layout for LTE. And the cache-oriented general techniques proposed earlier by researchers [8] do not find a straightforward application in our context, where the target system hardware consists of a coarse grained re-configurable array (CGRA) of SIMD functional units and vector registers.

The memory allocation problem has been studied before. 
However, we extend state of the art by proposing a more generic approach, which is also suitable for applications with input driven dynamic behaviour. 
The authors in \cite{Ben00b} present a methodology to generate a static application-specific memory hierarchy. 
Later, they extend their work in \cite{Ben00c} to a reconfigurable platform with multiple memory banks. 
However, our work differentiates by proposing a more generic and application agnostic methodology and employing the use of system scenarios, in order to efficiently handle a wider range of dynamic application characteristics. 

Several techniques for designing energy efficient memory architectures for embedded systems are presented in \cite{Mac02}. 
The current work differentiates by employing a platform that is reconfigurable at run-time. 
In \cite{Pgk01} a large number of data and memory optimisation techniques, that could be dependent or independent of a target platform, are discussed. 
Again, reconfigurable platforms are not considered.

Energy-aware assignment of data to memory banks for several task-sets based on the MediaBench suit of benchmarks is presented in \cite{Mar03}. 
Low energy multimedia applications are discussed also in \cite{Chu02} with focus on processing rather than the memory platform. 
Furthermore, both \cite{Mar03} and \cite{Chu02} base their analysis on use case situations and do not incorporate sufficient support for very dynamically behaving application codes. 
System scenarios alleviate this bottleneck and enable handling of such dynamic behaviour. 
In addition, the current work explores the assignment of data to the memory and the effect of different assignment decisions on the overall energy consumption.

The authors in \cite{abraham1999automatic}, \cite{jacob1996analytical} and \cite{li1999hardware} present methodologies for designing memory hierarchies.
Design methods with main focus on the traffic and latencies in the memory architecture are presented in \cite{chen1999loop}, \cite{grun2000mist}, \cite{jantsch1994hardware} and \cite{passes1995multi}.
Improving memory energy efficiency based on a study of access patterns is discussed in \cite{kandemir2001improving}.
Application specific memory design is a research topic in \cite{schmit1997synthesis}, while memory design for multimedia applications is presented in \cite{oshima1997high}.
The current work differentiates by introducing the concept of system scenarios that supports the dynamic handling of application's requirements, although the data mapping is static inside each scenario. 

An overview of work on system scenario methodologies and their application are presented in \cite{Gheorghita2007}. 
In \cite{Fil12} extensions towards a memory-aware system scenario methodology are presented and demonstrated using theoretical memory models and two target applications. 
This work is an extension both in complexity and accuracy of the considered memory library and on the number of target applications. 

Furthermore, the majority of the published work focus on control variables for system scenario prediction and selection. 
Control variables can take a relatively small set of different values and thus can be fully explored. However, the use of data variables \cite{Elena2010} is required by many dynamic systems including the majority of multimedia applications. 
The range of possible values for data variables is wider and makes full exploration impossible. 

Authors in \cite{Pal06} present a technique to optimise memory accesses for input data dependent applications by duplicating and optimising the code for different execution paths of a control flow graph (CFG). 
One path or a group of paths in a CFG form a scenario and its memory accesses are optimised using global loop transformations (GLT). 
Apart from if-statement evaluations that define different execution paths, they extend their technique to include while loops with variable trip count in \cite{Pal06b}. 
A heuristic to perform efficient grouping of execution paths for scenario creation is analysed in \cite{Pal07}. 
Our work extends the existing solutions towards exploiting the presence of a distributed memory organisation with reconfiguration possibilities.

Reconfigurable hardware for embedded systems, including the memory architecture, is a topic of active research. 
An extensive overview of current approaches is found in \cite{Garcia}. 
The approach presented in this paper differentiates by focusing on the data-to-memory assignment aspects in the presence of a platform with dynamically configurable memory blocks. 
Moreover, many methods for source code transformations, and especially loop transformations, have been proposed in the memory management context. 
These methods are fully complementary to our focus on data-to-memory assignment and should be performed prior to our step. 

\section{System Design Exploration Work-flow}
\label{sec:methodology}

The systematic exploration of interleaving options and data to memory mapping possibilities is necessary.
The exploration space consists of the potential combinations between the different arrays and the different scratch-pad memory architectures, which combine memories of different types and sizes.
The overall work-flow of this work is presented in Fig.\ref{fig:workflow}. 
The first step of the methodology is the analysis of the application code. 
The methodology is applicable to any application.
However the applications that can benefit more from the proposed methodology are the ones with irregular access patterns.
The second step is the interleaving exploration, which explores all different option for re-arrangement of the data.
The result of the interleaving is a set of data with a reduced number of holes.
The next step is the mapping of the interleaved data set to the clustered memory architecture.

\begin{figure}
\centering
	\caption{Methodology steps}
	\label{fig:workflow}
	\includegraphics[scale = 0.5]{Images/Workflow.eps} 
\end{figure}

\subsection{Formal Model Representation of Access Pattern }

\begin{figure}
\centering
	\caption{Extraction of access pattern from application code}
	\label{fig:pattern}
	\includegraphics[scale = 0.5]{Images/AHpattern.eps} 
\end{figure}

A representation model for the access pattern is employed in order to formally present each step of the methodology.
The model presented in \cite{Ang13} is a generic model suitable for irregular iteration spaces on arrays.
The irregularities are created by the application code access statements in a conditional loop structure.

When an array element is accessed, during the code execution, is represented with an A(Access).
Otherwise, there is a hole in the access pattern represented with an H(Hole) as shown in Fig.\ref{fig:pattern}.
The sequence of accesses and holes is usually repeated periodically, because normally the loop conditions are not totally random.
Thus we can define the frequency of each access pattern.
The analysis of the application code results in the access patterns and their corresponding frequencies, which is the necessary input for the next step on the work-flow.

%\textit{Discussion about polyhedral and enumerative approaches}
%\textit{Analysis of A-H model based on Angeliki}
%\textit{Definition of algebra functions between access patterns}

\subsection{Data Interleaving Exploration}
Interleaving is a data layout transformation for combining the storage of multiple arrays, so that blocks of data from different arrays are stored contiguously, in order to achieve spatial locality in memory accesses.
By interleaving we are able to group the data to be accessed and thus reduce the number of memory accesses for accessing them.

The employed model of access pattern representation is accompanied with a suitable algebra presented in \cite{kritikakou2013phd}.
The operations defined help the designer explore different interleaving options.
The goal of the process is to rearrange the data in a more efficient way that increases the number of consequential accesses.
A simple example is presented in Fig.\ref{fig:algebra}.
Arrays $A$ and $B$ are interleaved by interchangeably storing an element of each array in the new array. 
The access patterns of arrays $A$ and $B$ are combined in a new access pattern, which has two accessed elements placed consequentially.
The use of access patterns and the theory for calculating the access patterns of different combinations enables an extensive exploration of the possible data interleaving options.

The impact of the data interleaving exploration on the number of memory accesses is significant.
When the accesses are irregular and the data are organized in index order, each memory access results in a small amount of useful data due to the presence of holes.
In contrast the re-organization of the data provides a sequence of useful data without many holes between them.
Thus, a single access to the memory results in a higher number of useful elements.
The overall number of memory accesses is reduced, as each access has a higher utilization.

This idea is illustrated in Fig.\ref{fig:algebra}.
The number of memory accesses for each case can be calculated by assuming that each memory access loads four array elements, i.e. the word length for the memory and bus architecture is four elements.
Each time an element from the array $A$ or $B$ is needed, the memory access returns four elements, from which one is the useful and the other three are not used by the running code.
On the case of the interleaved array, each memory access returns four elements, from which two are useful and two useless.
Thus, the number of overall memory access in the second case is reduced in half compared to the first case.

\begin{figure}
\centering
	\caption{Example of combination between two arrays and their access patterns}
	\label{fig:algebra}
	\includegraphics[scale = 0.5]{Images/Algebra.eps} 
\end{figure}
%\textit{Algorithm for exploring data interleaving}

\subsection{Data-to-Memory Mapping Exploration}

Given the input from the previous step, we explore the mapping of the interleaved data to the memory architecture.
A clustered memory organisation with up to five memory banks of varying sizes is explored. 
The limitation in the number of memory banks is necessary in order to keep the interconnection cost between the processing element (PE) and the memories constant through exploration of different architectures. 
The decision to use memory banks with varying sizes on the clustered memory organization increases the reconfiguration options and consequently the potential energy gains. 
In general, smaller memories are more energy efficient compared to larger memories banks. 
However, in some cases large memory banks are needed in order to fit the application data without the need for too many small memories causing complex interconnects. 
The goal is to use the most energy efficient banks to store the interleaved data.

The exploration space consists of different sizes and types of memory banks.
The goal is to make the optimal decision regarding the mapping of the data to the different memory banks.
The parts of the interleaved data that consist mostly of useful elements are mapped into memory banks with low energy per access.
The parts of the interleaved data that consist of access holes and rarely accessed elements are optimally mapped into memory banks with energy efficient retention states.
In both cases the size of  the memory banks should be adequate to fit the stored data but at the same time as small size as possible to avoid area and energy penalties.

\subsection{One way constraint propagation}

The decisions taken on the interleaving step affect the mapping options.
If the interleaving decisions lead to small compact sets of data, the mapping can be done on small energy efficient memory banks.
On the other hand, if the mapping exploration is performed first, the freedom for interleaving is reduced.
We split the decisions in two steps and the interleaving decisions are propagated as constraints on the mapping exploration phase. 

\section{Target Architecture}
\label{sec:platform}

%\textit{Analysis of architecture based on our previous papers}
A generic architecture is presented in Fig.\ref{fig:arch}.
The methodology explores different interleaving and data to memory mapping options for this reconfigurable architecture.
The scratchpad memory consists of up to five memory banks. 
The optimal sizes are found based on the sizes of the data after the interleaving exploration.
The vector FU is assumed to perform instructions on multiple data.
The explored data lengths are 4, 8 and 16 words.
Each word is an array elements in our case. 

\begin{figure}
\centering
	\caption{Exploration options and system knobs depending on a general platform architecture}
	\label{fig:arch}
	\includegraphics[scale = 0.5]{Images/Architecture.eps} 
\end{figure}

\subsection{Memory Models}

The dynamic memory organisation is constructed using commercially available SRAM memory models (MM).
For those models delay and energy numbers are derived from a commercial memory compiler.
In addition, experimental standard cell-based memories (SCMEM) \cite{Mei11}  are  considered for smaller memories due to their energy and area efficiency for reasonably small storage capacities, as argued in \cite{Mei10}. 
The standard cell-based memories are synthesized using Cadence RTL compiler for TSMC 40nm standard library. 
Afterwords, power simulations on the synthesized design are carried out using Synopsys PrimeTime, in order to obtain energy numbers.
Both MMs and SCMEMs can operate under a wide range of supply voltages, thus support different operating modes that provide an important exploration space.
\begin{itemize}
\item Active mode: The normal operation mode, in which the memory can be accessed at the maximum supported speed. The supply voltage is 1.1V. 
The dynamic and leakage power are higher compared to the other modes.
Only on active mode the data are accessible without time penalties, in contrast to light and deep sleep modes.
In this work all the memory accesses are performed in active mode. 
\item Light sleep mode: The supply voltage in this mode is lower than active with values around 0.7V. 
The access time of the memory is significantly higher than the access time in active mode. 
Switching to active mode can be performed with a negligible energy penalty and a small time penalty of a few clock cycles (less than 10). 
Data is retained.  
\item Deep sleep mode: The supply voltage is set to the lowest possible value that can be used without loss of data. 
This voltage threshold is expected to be lower for SCMEMs than MM models and can be as low as 0.3V. 
The number of clock cycles needed for switching to active mode is higher compared to sleep mode, typically in the range of 20 to 50 clock cycles depending on the clock speed. 
Consequently, the speed of the PE and the real-time constrains of the applications has to be taken into consideration when choosing light or deep sleep mode at a specific time.  
\item Shut down mode: Power-gating techniques are used to achieve near zero leakage power. 
Stored data is lost. 
The switch to active mode requires substantially more energy and time. 
However, switching unused memories to this mode, providing that their data are not needed in the future, results in substantial energy savings.
\end{itemize}  

Should I add tables as well?

\subsection{Function Unit Models}

We have used an extension of the DRESC compiler framework [10] for mapping the reference application to a CGRA architecture.
The CGRA part comprises one SIMD enabled functional units (FU) and a central vector register file which is also coupled to a VLIW processor. 
For efficient utilization of the vector FU, the register file has a wide interface (256 bit wide) with the scratchpad memory and allows testing for word length of 4, 8 and 16 elements, assuming that each array element is a 16 bit wide.  

\section{Applications}
\label{sec:applications}

The applications that benefit most from the proposed methodology are characterised by having irregular access patterns with holes.
The multimedia domain offers some suitable candidates for the presented methodology.
An overview of the tested benchmark applications is presented below:

\begin{enumerate}
\item Motivational example is the very simple example presented in Sec.~\ref{sec:motivational}. 
It uses four arrays that all have the same irregular access pattern, namely 1A-3H.
One element from each array is used to calculate an intermediate result on every loop iteration.
The interleaving is easy because the four arrays has the same pattern.
Further interleaving within the array can result into even longer sequences of useful elements.
The access pattern is repeated for every four elements (1A-3H), so the scaling for the different word lengths (4, 8 and 16) is expected to be good. 
\item SOR benchmark has a more irregular access pattern.
The interleaving exploration provides sequences of 3 or 6 sequential useful elements.
Thus, the utilization on the SIMD architecture is expected to be lower.
\item FFT benchmark has an irregular pattern during the access of the pilot matrices. 
The interleaving exploration for FFT is presented in \cite{sharma2013data}.
The number of matrices is higher and more interleaving options are present.
However, the interleaving cannot provide an acceptable solution for 16 sequential useful elements.
\item Motion estimation benchmark is a dynamic algorithm that results in different access patterns based on the identification of the moving objects. 
The static parts are not accessed resulting in holes in the accesses and the interleaving aims to minimize those parts.
The interleaving exploration provides alternatives for all the  tested word lengths. 
\end{enumerate}

\section{Results}
\label{sec:results}

%\begin{table}%
%\tbl{Normalized energy consumption\label{tab:results}}{
%\begin{tabular}{c c}
%\includegraphics[width=0.45\linewidth]{Images/Example.eps} & \includegraphics[width=0.45\linewidth]{Images/sor.eps} \\
%\includegraphics[width=0.45\linewidth]{Images/fft.eps} & 
%\includegraphics[width=0.45\linewidth]{Images/mest.eps} 
%\end{tabular}}
%\end{table}

The design exploration is applied to the chosen application benchmarks and energy numbers are derived based on the described target platform.
The energy numbers are calculated both for the memory and the SIMD architecture presented in Sec.\ref{sec:platform}.
Four approaches are explored and the corresponding energy consumption of each of them is calculated.

\begin{itemize}
\item \textit{No optimization.} 
In this case there is no interleaving exploration and the memory architecture consists of a large memory bank. All the data are mapped on the memory bank without any optimization. 
\item \textit{Memory Size Optimization.} 
In this case there is no interleaving exploration and the memory architecture consists of four memory banks.
The optimal size for the memory banks and the optimal mapping of the non-interleaved data on them is explored. 
The number of memory accesses is the same as in the previous approach.
However, the data is mapped on an efficient clustered memory architecture.
\item \textit{Interleaving optimization.} 
In this case there is the  interleaving exploration step is performed and the memory architecture consists of one large memory bank.
The optimal interleaving decision is found and applied to the data, so the locality of useful data is increased.
However, all the data are mapped on one large memory bank.
The number of accesses is significantly reduced by the interleaving step, but the energy per access is kept high due to lack of data mapping to an efficient clustered architecture.
\item \textit{Integrated co-exploration.} 
In this case the co-exploration of both the interleaving and the data-to-memory mapping optimizations are performed.
Both he number of memory accesses and the energy per access are reduced.
\end{itemize}

\begin{figure}
\centering
	\caption{Motivational Example}
	\label{fig:example}
	\includegraphics[scale = 0.5]{Images/Example.eps} 
\end{figure}

The normalized energy consumption for the motivational example is presented in Fig.\ref{fig:example}.
The four different approaches are normalized using the conventional case without any optimization.
Energy results for this benchmark are presented for a word length of 4, 8 and 16 elements, which means that every memory access loads/stores 4,8 or 16 elements. 
The application code is perfectly suitable for interleaving as discussed in Sec.\ref{sec:motivational}.
Thus the interleaving exploration has a greater impact that the memory size optimization.
However, the integrated approach optimizes the energy consumption even further.
The application is suitable for higher word lengths and there are important gains while moving from 4 to 8 and 16.
This is explained by the nature of the application that offers good interleaving options for larger words.

\begin{figure}
\centering
	\caption{SOR Benchmark}
	\label{fig:sor}
	\includegraphics[scale = 0.5]{Images/sor.eps} 
\end{figure}

The normalized energy consumption for the SOR benchmark is presented in Fig.\ref{fig:sor}.
The four different approaches are normalized using the conventional case without any optimization.
Energy results for this benchmark are presented for a word length of 3 and 6, which means that every memory access loads/stores 4 or 8 elements, because the architecture supports word lengths in the power of 2. 
The interleaving exploration on the application code concludes that it is only possible to make sequential sets of 3 and 6 elements by interleaving the arrays of the application.
Thus the interleaving exploration alone has a small impact on the reduction of energy consumption for word length of 3 elements.
The results are even worst for a word length of 6 elements, because the interleaved data have greater size and the conventional mapping of them to the memory is poor.
The memory size optimization provides more important reduction.
The integrated approach exploit on a better way the few interleaving options by providing a better mapping of the interleaved data to the memory architecture.

\begin{figure}
\centering
	\caption{FFT Benchmark}
	\label{fig:fft}
	\includegraphics[scale = 0.5]{Images/fft.eps} 
\end{figure}

The normalized energy consumption for the FFT benchmark is presented in Fig.\ref{fig:fft}.
The four different approaches are normalized using the conventional case without any optimization.
Energy results for this benchmark are presented for a word length of 4 and 8 elements, which are the two viable options provided by the interleaving exploration.
Again, the integrated approach results in the lower energy consumption.
The energy gains for a word length of 8 are significantly high, because blocks of 8 words can be constructed by interleaving of application's arrays.

\begin{figure}
\centering
	\caption{Motion Estimation Benchmark}
	\label{fig:mest}
	\includegraphics[scale = 0.5]{Images/mest.eps} 
\end{figure}

The normalized energy consumption for the motion estimation benchmark is presented in Fig.\ref{fig:mest}.
The four different approaches are normalized using the conventional case without any optimization.
Energy results for this benchmark are presented for a word length of 4, 8 and 16 elements.
The application code provides good possibilities for interleaving and consequentially the interleaving exploration has a greater impact that the memory size optimization.
However, the integrated approach optimizes the energy consumption even further.
In this case the energy gains for increasing size of word length are minimal.
This is explained by the nature of the application...

\section{Conclusion}
\label{sec:conclusion}

Justify why the integrated approach gives better results than just combining the two methods...
% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{reference}

\end{document}